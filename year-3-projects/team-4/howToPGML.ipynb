{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "howToPGML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaBx92eW2QaQ",
        "colab_type": "text"
      },
      "source": [
        "# Using an existing NN on new data\n",
        "There are only a few steps to getting your data ready to be pushed into the network. I realize that there are a lot of different files on taki and it is not very obvious on how things should be done.\n",
        "\n",
        "As such I found it easier to just create this notebook to demonstrate how to use everything and what kind of functions you'll need to generate a confusion matrix like table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDa8KefY24yw",
        "colab_type": "text"
      },
      "source": [
        "## Download the required libraries running all parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa0gS_7w2N_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install \"tensorflow-gpu>=2.0.0\" pandas matplotlib dill multiprocessing_on_dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwZVbZdV3SGh",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the neural network\n",
        "In order to setup the neural network the first thing is to download it from my GDrive. Ideally you would upload the network and keep it private to your drive if you wanted to run this notebook.\n",
        "\n",
        "This will also allow us to load in the scaler object.\n",
        "\n",
        "I'm using a self-compiled adaption of https://github.com/gdrive-org/gdrive to upload things from my computer directly to GDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSgTnG-72-qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_path = 'gdrive/My Drive/'\n",
        "\n",
        "\n",
        "def download_model(u):\n",
        "  x = requests.get(u, stream=True, allow_redirects=True)\n",
        "  print(\"Downloaded a\",x.headers.get('content-type'))\n",
        "  x.raw.decode_content = True # handle spurious Content-Encoding\n",
        "  y = x.raw.read()\n",
        "  # Dumb work around because Keras won't load from raw bytes\n",
        "  with open(\"tmp.h5\", \"wb\") as outfile:\n",
        "    outfile.write(y)\n",
        "  model = load_model(\"tmp.h5\")\n",
        "  return model\n",
        "def download_py(filename, u):\n",
        "  x = requests.get(u, stream=True, allow_redirects=True)\n",
        "  print(\"Downloaded a\",x.headers.get('content-type'))\n",
        "  x.raw.decode_content = True # handle spurious Content-Encoding\n",
        "  y = x.raw.read()\n",
        "  # Dumb work around because Keras won't load from raw bytes\n",
        "  with open(filename, \"wb\") as outfile:\n",
        "    outfile.write(y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYG1auX63_yQ",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Input Data\n",
        "Now we have to load in the scaler object and put it into the same features ranges as the data which the network was trained on.\n",
        "\n",
        "I will assume that your data is RAW and not at all preprocessed.\n",
        "1. There are no extra labels\n",
        "2. It is in a gross CSV\n",
        "3. You know the outputs and want to evaluate accuracy\n",
        "\n",
        "Each subsection goes over how put it into the shape we desire! We are assuming that you are NOT attempting to generate false singles, false doubles, swapped rows, or anything like that. That process takes a very long time on taki and it would never run on colab.\n",
        "\n",
        "1. If you're just trying to do prediction and do not have encoded data proceed through preprocessing as normal but skip \"Getting a confusion matrix\". Set `outputs=False`.\n",
        "2. If you're trying to use preencoded data which is non-normalized just skip to \"One hot encoding\" and load your data there and proceed as normal.\n",
        "3. If you're trying to use pre-encoded normalized data just skip to \"One hot encoding\" and name your inputs variable `xscale` instead of `x`.\n",
        "4. If you want to use non-encoded data :\n",
        " 1. If it has already been sanitized and labelled you can either edit the load command in \"Convert from CSV to DF\" then simply skip \"Generating the Maggi Labels\" and also skip \"Adding the Permutation Labels\". \n",
        " 2. You can also just edit the load command in \"One hot encoding\". Load your CSV into `all_data`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1OcDqPpWJpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPyv0NvBHCYm",
        "colab_type": "text"
      },
      "source": [
        "### Required helper functions\n",
        "\n",
        "Run the below block to initialize all helper functions required.\n",
        "\n",
        "We'll download the source files from taki to save on space notebook space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njxbHxmDG66F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from shutil import copy\n",
        "files = [\"convertToOneHot.py\", \"maggi.py\", \"makeNewLabels.py\", \"helpers.py\"]\n",
        "driveLocation = \"colabFiles\"\n",
        "for aFile in files:\n",
        "  copy(\"{}/{}/{}\".format(root_path, driveLocation, aFile), \"./{}\".format(aFile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHViEiY5SqN",
        "colab_type": "text"
      },
      "source": [
        "### Converting From CSV to DF\n",
        "Our first goal is to take in the original CSV and load it into a `pandas.DataFrame` so we can easily add all labels to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcKSasOA9y56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputFile = \"{}/{}\".format(root_path, \"PGML_DATA/1GEvents_150MeV_sp_E0_nulls.csv\")\n",
        "import pandas as pd\n",
        "detM = pd.read_csv(inputFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Unza2i5hBh",
        "colab_type": "text"
      },
      "source": [
        "### Generating the Maggi Labels\n",
        "First we'll used an updated version of Dr. Maggi's preprocessing script to add his original labels which are then used to build the newer labels.\n",
        "\n",
        "Note that in more time constrained scenario we would jump straight to One hot encoding after adding Maggi Labels.\n",
        "However, for now, we do this a little slower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ3yJxUT9zjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from maggi import addMaggiLabels\n",
        "lowEThresh = 0.05\n",
        "upperEThresh = 2.7\n",
        "sings, doubs, trips = addMaggiLabels(detM, lowEThresh, upperEThresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnKtmg4c5uRD",
        "colab_type": "text"
      },
      "source": [
        "### Adding the Permutation Labels\n",
        "Now we add all the newer labels! In the actual code on taki, for gigantic files, this is done using some parallel magic. Please note that the processors on Colab are terrible! You're better off loading pre-encoded data at an earlier stage and just skipping this step if you can.\n",
        "\n",
        "I'll optimize the routines later...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVtc3zyE90rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from makeNewLabels import computePermuteLabels\n",
        "print(\"Computing permutation labels for doubles\")\n",
        "doubs = doubs.apply(lambda row: computePermuteLabels(row, outputs=outputs), axis=1)\n",
        "print(\"Computing permutation labels for triples\")\n",
        "trips = doubs.apply(lambda row: computePermuteLabels(row, outputs=outputs), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFkDbhh50Q7",
        "colab_type": "text"
      },
      "source": [
        "### One hot encoding\n",
        "Now we'll take the large DataFrame and one hot encode it so that it can be used in the network.\n",
        "\n",
        "1. If you have data which has aleady been encoded feel free to just comment out all of the code in the next block. Just load your unscaled data into `x` \n",
        "2. if you have known outputs put them into `y`. \n",
        "3. If you do not have known outputs just set `y` to be an empty array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVLkrxxl91aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import concat, read_csv\n",
        "from convertToOneHot import toOneHotArray\n",
        "from numpy import array\n",
        "# all_data = concat([trips, doubs])\n",
        "all_data = read_csv(\"{}/{}/{}\".format(root_path, driveLocation, \"all_data_05G.csv\"))\n",
        "x, y = toOneHotArray(all_data, outputs=outputs)\n",
        "# x = None\n",
        "# y = array([])\n",
        "# xscale = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_dGzDx7oTp",
        "colab_type": "text"
      },
      "source": [
        "### Normalization\n",
        "From here we will normalize your encoded data which finalizes its usability for the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H4ELzbP9-UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dill\n",
        "with open(\"{}/{}/{}\".format(root_path, driveLocation, \"all_data_40G_balanced.scaler\"), \"rb\") as infile:\n",
        "  scaler = dill.load(infile)\n",
        "xscaled = scaler.transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh1LG6CK4NaW",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating Network Performance\n",
        "Here we compute the confusion matrix and also evaluate the NN performance on the data you've provided. If you're using data that does not have a known output then you should expect `y` to be an empty array. As such do not bother running through \"Getting a confusion matrix\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyuDq3aF77gw",
        "colab_type": "text"
      },
      "source": [
        "### Required functions\n",
        "This block loads the model and generates the two indexing dictionaries which will be needed for the following two blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VQiyBf_-BZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index1 = {'123': '0', '124': '1', '132': '2', '134': '3', '142': '4', '143': '5',\n",
        "'213': '6', '214': '7', '231': '8', '234': '9', '241': '10', '243': '11', \n",
        "'312':'12', '314': '13', '321': '14', '324': '15', '341': '16', '342': '17', \n",
        "'412': '18', '413': '19', '421': '20', '423': '21', '431': '22', '432': '23', \n",
        "'444':'24'}\n",
        "# Lazy\n",
        "index2 = {}\n",
        "for key in index1.keys():\n",
        "  index2[int(index1[key])] = key\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"{}/{}/{}\".format(root_path, driveLocation, \"15dd8d785eb6f701fd1eda81572657bcb22a4fe76fa7b41672b1a60b5ea70e91-1024.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFv9OY8Z8EN4",
        "colab_type": "text"
      },
      "source": [
        "### Getting a confusion matrix\n",
        "Here we evaluate the network's performance on your provided data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeQOHQdm-CO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import DataFrame, set_option\n",
        "from numpy import max, argmax, concatenate, where, zeros, arange, nan_to_num\n",
        "set_option('display.max_columns', None)\n",
        "set_option('display.max_rows', None)\n",
        "def getConfusionMatrix(model, x, y):\n",
        "  # Get predictions\n",
        "  y_res = model.predict(x)\n",
        "  y_max = max(y_res, axis=1).reshape(y_res.shape[0],1)\n",
        "  # Convert prediction to binary classifcation\n",
        "  y_res[y_res < y_max] = 0\n",
        "  y_res[y_res >= y_max] = 1\n",
        "  # Compare predicted results to real answers\n",
        "  t = argmax(y, axis=1).reshape(y.shape[0], 1)\n",
        "  # 25 classes classified 25 ways\n",
        "  v = zeros((25,25))\n",
        "  for i in range(25):\n",
        "    # Select only rows for things which correctly classify to i\n",
        "    z1, z2 = where(t == i)\n",
        "    # Determine how many times (i) was classified into each class\n",
        "    v[i,:] = y_res[z1].sum(axis=0)\n",
        "  # Convert the raw values into percents of total\n",
        "  vs = v.sum(axis=1).reshape(v.shape[0],1)\n",
        "  # To prevent NaN\n",
        "  vs[vs == 0] = 1\n",
        "  percent = (v / vs)\n",
        "  percent = nan_to_num(percent.flatten()).reshape(percent.shape)\n",
        "  columns = list(map(lambda x: index2[x], sorted(index2.keys())))\n",
        "  # columns.insert(0, \"correct\")\n",
        "  d = DataFrame(v, index=columns, columns=columns)\n",
        "  percent = DataFrame(percent, index=columns, columns=columns)\n",
        "  return d, percent\n",
        "raw, per = getConfusionMatrix(model, xscaled, y)\n",
        "%load_ext google.colab.data_table\n",
        "per\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}